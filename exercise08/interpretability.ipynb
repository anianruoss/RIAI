{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import io\n",
    "\n",
    "import cv2\n",
    "import matplotlib.cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import lucid\n",
    "import lucid.modelzoo.vision_models as models\n",
    "import lucid.optvis.objectives as objectives\n",
    "import lucid.optvis.param as param\n",
    "import lucid.optvis.render as render\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.InceptionV1()\n",
    "model.load_graphdef()\n",
    "\n",
    "# download a mapping of the imagenet class ids to text\n",
    "imagenet_classes_request = requests.get(\"https://gist.githubusercontent.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57/raw/aa66dd9dbf6b56649fa3fab83659b2acbf3cbfd1/map_clsloc.txt\")\n",
    "imagenet_classes_list = [None] + [a.split(' ')[2] for a in str(imagenet_classes_request.content).split('\\\\n')]\n",
    "\n",
    "# download an image from flickr\n",
    "response = requests.get(\"http://c1.staticflickr.com/5/4070/5148597478_0c34ec0b7e_n.jpg\")\n",
    "image = Image.open(io.BytesIO(response.content))\n",
    "image = np.array(image, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a k-NMF of data A\n",
    "def nfm(k, A):\n",
    "    factorizer = NMF(n_components=k)\n",
    "    shape = A.shape\n",
    "    nmf_shape = (np.prod(shape[:-1]), shape[-1])\n",
    "    U = factorizer.fit_transform(A.reshape(nmf_shape))\n",
    "    V = factorizer.components_\n",
    "    U = U.reshape(shape[1:-1] + (k,) )\n",
    "    V = V.reshape((k,shape[-1]) )\n",
    "    \n",
    "    return U, V\n",
    "\n",
    "# compute the necessary forward and backward passes on a model in order to obtain\n",
    "# the activations at the desired layer, the gradients for specific logits\n",
    "# and the evaluation of the logits\n",
    "# activations is a tensor with the actions at layer `layer`\n",
    "# grads is a list where each entry is a tensor with the gradient at layer `layer\n",
    "# from the class-logit with the same position in the array classes\n",
    "def foward_pass_and_gradients(model, image, classes, layer):\n",
    "    with tf.Graph().as_default(), tf.Session():\n",
    "        t_input = tf.placeholder_with_default(image, [None, None, 3])\n",
    "        T = render.import_model(model, t_input, t_input)\n",
    "        activations = T(layer).eval()\n",
    "        logits = T(\"softmax2_pre_activation\")\n",
    "        grads = []\n",
    "        for c in classes:\n",
    "            t_grad = tf.gradients([logits[0, c]], [T(layer)])[0]   \n",
    "            grad = t_grad.eval({T(layer) : activations})\n",
    "            grads.append(grad)\n",
    "        return activations, grads, logits.eval() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_group(model, layer, weights, color, color_strip_height=0.1, visualization_size=100, diversity=1, lambda_diversity=1e3):    \n",
    "    color_strip_height = max(int(color_strip_height * visualization_size), 1)\n",
    "    color_strip = np.ones((color_strip_height, visualization_size, 3)) * color\n",
    "    param_f = lambda: param.image(visualization_size, batch=diversity)\n",
    "    obj = objectives.direction(layer, weights.ravel())\n",
    "    if diversity > 1:\n",
    "         obj -= lambda_diversity*objectives.diversity(layer)\n",
    "    visualization = render.render_vis(model, obj, param_f, verbose=False)[-1]\n",
    "    visualization = list(map(lambda x: x[0, ...], np.split(visualization, diversity)))\n",
    "    print(\".\", end='', flush=True) # progress indicator as this step is slow\n",
    "    visualization[0] = np.vstack([color_strip, visualization[0]])\n",
    "    return visualization\n",
    "\n",
    "def get_group_colors(k):\n",
    "    colors = matplotlib.cm.inferno.colors\n",
    "    return [np.array(colors[int((j+1)*(256-1)/(k+1))]) for j in range(k)]\n",
    "\n",
    "def get_saliency_map(groups, colors, treshold):\n",
    "    k = groups.shape[-1]\n",
    "    saliency_map = np.zeros(groups.shape[:-1] + (4,))\n",
    "    for j in range(k):  \n",
    "        c = colors[j]\n",
    "        intensities = groups[:, :, j] / groups[:, :, j].max()\n",
    "        cmap = np.tile(np.expand_dims(intensities, -1), (1, 1, 3))\n",
    "        cmap *= np.tile(c[None, None, :], groups.shape[:-1] + (1,))\n",
    "        cmap[groups[:, :, j] < treshold * groups[:, :, j].max()] = 0\n",
    "        saliency_map[:, :, :3] += cmap\n",
    "        saliency_map[:, :, 3] = np.maximum(intensities, saliency_map[:, :, 3])\n",
    "    return saliency_map\n",
    "\n",
    "def compute_effects(groups, weights, gradients):\n",
    "    k = groups.shape[-1]\n",
    "    nr_classes = len(gradients)\n",
    "    effects = np.zeros((nr_classes, k))\n",
    "    for j in range(k):\n",
    "        groupweights = groups[:, :, j, None] * weights[j, :]\n",
    "        for i in range(nr_classes):\n",
    "            effects[i, j] = np.sum(groupweights * gradients[i][0, ...])\n",
    "    return effects\n",
    "\n",
    "def show(image, saliency_map, group_visualizations, layer):\n",
    "    k = len(group_visualizations)\n",
    "    d = len(group_visualizations[0])\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    f.suptitle(layer + ' visualized with ' + str(k) + ' groups', fontsize=16)\n",
    "    axarr[0].axis('off')\n",
    "    axarr[0].imshow(image/255.0)    \n",
    "    axarr[1].axis('off')\n",
    "    axarr[1].imshow(saliency_map)    \n",
    "    axarr[2].axis('off')\n",
    "    axarr[2].imshow(image/255.0)    \n",
    "    saliency_map_large = cv2.resize(saliency_map, dsize=(image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    axarr[2].imshow(saliency_map_large)\n",
    "    f, axarr = plt.subplots(d, k, figsize=(18, 4*d))\n",
    "    axarr = axarr.reshape((d, k))\n",
    "    for j in range(k):\n",
    "        for k in range(d):\n",
    "            axarr[k, j].axis('off')\n",
    "            axarr[k, j].imshow(group_visualizations[j][k])\n",
    "\n",
    "def print_effects(effects, target_classes):\n",
    "    _, k = effects.shape\n",
    "    print('Effects:')\n",
    "    print('Class/Group', end='\\t')\n",
    "    for j in range(k):\n",
    "        print(j, end='\\t')    \n",
    "    print()\n",
    "    for s, classname in enumerate(target_classes):\n",
    "        print(classname, end='\\t')\n",
    "        for j in range(k):\n",
    "            print(effects[s, j].round(2), end='\\t')\n",
    "        print()\n",
    "\n",
    "def building_blocks_of_interpretability(model, image, layer, classes, imagenet_classes_list, k=5,\n",
    "                                        saliency_map_treshold=0.7, visualization_size=100, diversity=1,\n",
    "                                        lambda_diversity=1e3):\n",
    "    layer_names = list(map(lambda x: x.name, model.layers))\n",
    "    assert 1 <= k\n",
    "    assert layer in layer_names\n",
    "    classes_ids = list(map(lambda x: imagenet_classes_list.index(x), classes))\n",
    "    activations, gradients, logits = foward_pass_and_gradients(model, image, classes_ids, layer)\n",
    "    top_classes = logits[0, :].ravel().argsort()[::-1][:10]\n",
    "    print('Top 10 classes:')\n",
    "    print(list(map(lambda x: imagenet_classes_list[x], top_classes.tolist())))\n",
    "    print('Computing NMF')\n",
    "    groups, weights = nfm(k, activations)\n",
    "    print('Visualizing Groups')\n",
    "    colors = get_group_colors(k)\n",
    "    group_visualizations = [visualize_group(model, layer, weights[j, :], colors[j], visualization_size=visualization_size, diversity=diversity, lambda_diversity=lambda_diversity) for j in range(k)]\n",
    "    print()\n",
    "    print('Creating Saliency Map')\n",
    "    saliency_map = get_saliency_map(groups, colors, saliency_map_treshold)\n",
    "    print('Computing effects')\n",
    "    effects = compute_effects(groups, weights, gradients)\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    show(image, saliency_map, group_visualizations, layer)\n",
    "    print_effects(effects, classes)   \n",
    "    \n",
    "    return group_visualizations, colors, saliency_map, effects\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you are running on a CPU visualzing a 60x60 image takes roughly 1 or 2 minutes on a modern laptop. So during developement/debugging you will want to keep k and the visualization_size small or move to a GPU (for example move the notebook to colab, where you did the lucid tutorials, which allows free GPU access for 2 hour sessions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = list(map(lambda x: x.name, model.layers))\n",
    "print(layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'mixed5a'\n",
    "target_classes = ['bloodhound', 'tiger_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_visualizations, colors, saliency_map, effects =\\\n",
    "building_blocks_of_interpretability(model, image, layer, target_classes, imagenet_classes_list,\n",
    "                                    k=2, visualization_size=20, diversity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_visualizations, colors, saliency_map, effects =\\\n",
    "building_blocks_of_interpretability(model, image, layer, target_classes, imagenet_classes_list,\n",
    "                                    k=5, visualization_size=160, diversity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_visualizations, colors, saliency_map, effects =\\\n",
    "building_blocks_of_interpretability(model, image, layer, target_classes, imagenet_classes_list,\n",
    "                                    k=5, visualization_size=160, diversity=4, lambda_diversity=1e4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
